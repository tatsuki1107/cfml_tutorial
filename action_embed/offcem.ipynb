{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e179011-a267-4013-92b2-f84636ab90e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RandomForestRegressor' from 'sklearn.tree' (/usr/local/lib/python3.9/site-packages/sklearn/tree/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mobp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mope\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RegressionModel\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor \u001b[38;5;28;01mas\u001b[39;00m RFR\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msynthetic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SyntheticBanditDatasetWithCluster\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'RandomForestRegressor' from 'sklearn.tree' (/usr/local/lib/python3.9/site-packages/sklearn/tree/__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from obp.ope import RegressionModel\n",
    "from sklearn.tree import RandomForestRegressor as RFR\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset.synthetic import SyntheticBanditDatasetWithCluster\n",
    "from ope.regression import PairWiseRegression\n",
    "from ope.meta import OffPolicyEvaluation\n",
    "from ope.estimator import InversePropensityScore as IPS\n",
    "from ope.estimator import MarginalizedIPS as MIPS\n",
    "from ope.estimator import DoublyRobust as DR\n",
    "from ope.estimator import OFFCEM\n",
    "from utils.common import visualize_mean_squared_error\n",
    "from utils.common import aggregate_simulation_results\n",
    "from policy.function import gen_eps_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "893d0854-be67-435c-9180-2bcefcc14fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setting\n",
    "n_users=100\n",
    "dim_context=10\n",
    "n_actions=100\n",
    "n_cat_per_dim=3\n",
    "n_cat_dim=10\n",
    "n_clusters=30\n",
    "beta=-1.0\n",
    "eps=0.3\n",
    "reward_noise=1.0\n",
    "random_state=12345\n",
    "n_sim=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da5ecf12-c3ee-44f0-b063-a1c0d5e1416a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MSE with varying sample sizes in logged data\n",
    "\n",
    "sample_sizes = [1000, 3000, 6000, 12000]\n",
    "ope_estimators = [\n",
    "    IPS(estimator_name=\"IPS\"),\n",
    "    DR(estimator_name=\"DR\"),\n",
    "    MIPS(estimator_name=\"MIPS (true)\"),\n",
    "    OFFCEM(estimator_name=\"OFFCEM\"),\n",
    "    OFFCEM(estimator_name=\"OFFCEM + 1-step reg\"),\n",
    "    OFFCEM(estimator_name=\"OFFCEM (LC)\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed28fbb-445f-4816-9808-72a5fc516fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val_size=1000:   0% 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = SyntheticBanditDatasetWithCluster(\n",
    "    n_users=n_users,\n",
    "    dim_context=dim_context,\n",
    "    n_actions=n_actions,\n",
    "    n_cat_per_dim=n_cat_per_dim,\n",
    "    n_cat_dim=n_cat_dim,\n",
    "    n_clusters=n_clusters,\n",
    "    beta=beta,\n",
    "    reward_noise=reward_noise,\n",
    "    random_state=random_state\n",
    ")\n",
    "n_clusters = dataset.n_clusters\n",
    "\n",
    "test_data = dataset.obtain_batch_bandit_feedback(n_rounds=30000)\n",
    "policy_value = dataset.calc_ground_truth_policy_value(\n",
    "    q_x_a=test_data[\"expected_reward\"],\n",
    "    pi_e=gen_eps_greedy(expected_reward=test_data[\"expected_reward\"], eps=eps)\n",
    ")\n",
    "\n",
    "result_df_list = []\n",
    "for val_size in sample_sizes:\n",
    "    \n",
    "    result_list = []\n",
    "    for _ in tqdm(range(n_sim), desc=f\"val_size={val_size}\"):\n",
    "        val_data = dataset.obtain_batch_bandit_feedback(n_rounds=val_size)\n",
    "        \n",
    "        pi_e = gen_eps_greedy(\n",
    "            expected_reward=val_data[\"expected_reward\"],\n",
    "            eps=eps,\n",
    "        )\n",
    "        # off policy evaluation\n",
    "        ope = OffPolicyEvaluation(\n",
    "            bandit_feedback=val_data,\n",
    "            ope_estimators=ope_estimators,\n",
    "        )\n",
    "        \n",
    "        ## train_reward_via_two_stage\n",
    "        ### 1st-stage\n",
    "\n",
    "        pairwise_model = PairWiseRegression(\n",
    "            dim_context=dim_context,\n",
    "            n_actions=n_actions,\n",
    "            n_clusters=n_clusters,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        h_hat = pairwise_model.fit_predict(bandit_data=val_data)\n",
    "        \n",
    "        ### 2st-stage\n",
    "        reward = val_data[\"reward\"]\n",
    "        reward_residual = reward - h_hat[np.arange(val_size), val_data[\"action\"]]\n",
    "        cluster, phi_x_a = val_data[\"cluster\"], val_data[\"phi_x_a\"]\n",
    "\n",
    "        reg_model = RegressionModel(\n",
    "            n_actions=n_clusters,\n",
    "            action_context=np.eye(n_clusters),\n",
    "            base_model=MLP(hidden_layer_sizes=(10, 10, 10), random_state=random_state)\n",
    "        )\n",
    "\n",
    "        g_hat = reg_model.fit_predict(\n",
    "            context=val_data[\"context\"],\n",
    "            action=cluster,\n",
    "            reward=reward_residual\n",
    "        )[:, :, 0]\n",
    "\n",
    "        f_hat_x_a_e = h_hat + g_hat[np.arange(val_size)[:, None], phi_x_a]\n",
    "        \n",
    "        ## one-step reward regression\n",
    "        reg_model = RegressionModel(\n",
    "            n_actions=n_actions,\n",
    "            action_context=val_data[\"action_context_one_hot\"],\n",
    "            base_model=MLP(hidden_layer_sizes=(10, 10, 10), random_state=random_state),\n",
    "        )\n",
    "        q_hat_x_a = reg_model.fit_predict(\n",
    "            context=val_data[\"context\"],\n",
    "            action=val_data[\"action\"],\n",
    "            reward=val_data[\"reward\"],\n",
    "        )\n",
    "        \n",
    "        q_hat_dict = {\n",
    "            \"DR\": q_hat_x_a,\n",
    "            \"OFFCEM\": f_hat_x_a_e,\n",
    "            \"OFFCEM + 1-step reg\": q_hat_x_a,\n",
    "            \"OFFCEM (LC)\": val_data[\"expected_reward\"]\n",
    "        }\n",
    "        \n",
    "        estimated_policy_values = ope.estimate_policy_values(action_dist=pi_e, estimated_rewards=q_hat_dict)\n",
    "        result_list.append(estimated_policy_values)\n",
    "    \n",
    "    # calculate MSE\n",
    "    result_df = aggregate_simulation_results(\n",
    "        simulation_result_list=result_list, policy_value=policy_value, x_value=alpha\n",
    "    )\n",
    "    result_df_list.append(result_df)\n",
    "\n",
    "result_df = pd.concat(result_df_list).reset_index(level=0)\n",
    "visualize_mean_squared_error(\n",
    "    result_df=result_df,\n",
    "    xlabel=\"sample sizes in logged data\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
